---
layout: page
title: Use It Yourself
description: The constraint stacking method as a toolkit. Apply it to your own questions.
permalink: /toolkit/
---

## Take the Method

The method used on this site isn't proprietary. Here's how to apply it to any hard question.

---

## Core Principle

**We don't argue FOR conclusions. We eliminate AGAINST explanations.**

The question isn't "What do I believe?" — it's "What survives when I throw everything at it?"

---

## The Basic Process

### 1. Define the Question Precisely

Bad: "What's the deal with consciousness?"
Good: "Does the brain produce consciousness, or is consciousness fundamental?"

The question should be testable. Both sides should have evidence that could support them.

### 2. List All Plausible Explanations

Include everything reasonable people might believe:
- The mainstream view
- The heterodox view
- Edge cases and weird possibilities
- "We don't know" as an explicit option

Don't pre-filter. The constraints will do the filtering.

### 3. Gather Evidence and Order by Eliminative Power

**Most damning first.** Evidence that eliminates the most explanations goes at the top.

Hierarchy of evidence types (strongest to weakest):
1. **Unexplained anomalies that MUST exist** — Things that shouldn't happen if a theory is true, but do happen
2. **Methodological limits** — "We can't explain X yet" (weaker than "X violates our theory")
3. **Absence of evidence** — Not finding something (weakest — absence isn't proof)
4. **Prior probability** — "It seems unlikely" (almost useless alone)

### 4. Apply Constraints Top-Down

For each piece of evidence, ask: "Which explanations does this eliminate?"

Mark eliminations clearly:
- ❌ Eliminated — Cannot survive this constraint
- ⚠️ Damaged — Weakened but not killed
- ✓ Survives — Consistent with this evidence

### 5. Report What Survives

After all constraints are applied, what's left standing?

Categories of results:
- **Eliminated** — Cannot be true given the evidence
- **Cannot eliminate** — Survives all constraints (doesn't mean "proven true")
- **Unresolved** — Evidence is ambiguous or insufficient

---

## Critical Refinements

### Two-Sided Testing

**Always test both directions when the question has two sides.**

Wrong approach:
> "Let's test if materialism is true" → Find problems → Conclude materialism is false

Right approach:
> "Let's test materialism against evidence" → Find problems
> "Now let's test non-materialism against evidence" → Find problems
> "Which side has worse problems?"

This prevents confirmation bias. You might find that your preferred view has fatal problems too.

### The Weighing

When both sides have problems, weigh by evidence type:

| If Side A has... | And Side B has... | Then... |
|------------------|-------------------|---------|
| Unexplained anomalies | Methodological limits | A has worse problems |
| Methodological limits | Absence of evidence | A has worse problems |
| Absence of evidence | Prior discomfort | A has worse problems |

Example: Materialism has unexplained anomalies (terminal lucidity, hard problem). Non-materialism has methodological limits (no mechanism). Unexplained anomalies are worse than methodological limits.

### Control Groups

**Include cases where you expect mundane answers.**

If your method finds exotic conclusions everywhere, it's biased. If it finds exotic conclusions only where evidence warrants, it's calibrated.

Example cases that SHOULD debunk:
- Bermuda Triangle (no statistical anomaly exists)
- Most conspiracy theories (simpler explanations work)
- Cold reading "psychics" (known techniques explain it)

If your method can't debunk these, something's wrong.

### Stating Uncertainty Honestly

Some results:
- **Cannot eliminate** — Not the same as "proven true"
- **Unresolved** — The evidence is genuinely ambiguous
- **Unknown** — We don't have enough evidence either way

Stating limits clearly STRENGTHENS the overall picture. If you claim certainty everywhere, you're not doing epistemics.

---

## Template for a Single Analysis

```markdown
## [QUESTION]

**Claim being tested:** [Precise statement]

**Solution space:**
- Option A
- Option B
- Option C
- Unknown

---

### CONSTRAINT 1: [Most Damning Evidence]

**Evidence:** [What specifically]

**Eliminates:**
- ❌ Option A — [Why]
- ❌ Option B — [Why]

**Remaining:** Option C, Unknown

---

### CONSTRAINT 2: [Next Most Damning]

**Evidence:** [What specifically]

**Eliminates:**
- ❌ Option C — [Why]

**Remaining:** Unknown? Or does something survive?

---

[Continue until solution space is maximally narrowed]

---

## PART 2: Testing the Survivor

[Apply constraints to whatever survived Part 1]

**What problems does the surviving view face?**

[Be honest about weaknesses]

---

## Result

| Hypothesis | Status |
|------------|--------|
| Option A | ❌ Eliminated |
| Option B | ❌ Eliminated |
| Option C | Cannot eliminate |

---

## What This Means

[Interpretation, connections, implications]

## What Would Challenge This

[Specific findings that would change the result]
```

---

## Common Failure Modes

### 1. One-Sided Testing
Only looking for evidence against the view you dislike. Fix: Test both directions.

### 2. Counting Arguments Instead of Weighing
"I have 10 points against, you have 5" — irrelevant. One unexplained anomaly outweighs ten vague concerns. Fix: Use the evidence-type hierarchy.

### 3. Treating Absence as Evidence
"No one has proven X, therefore not-X." Absence of evidence is weak. Fix: Distinguish absence from anomaly.

### 4. Smuggling Conclusions Into Premises
"Assuming consciousness is produced by the brain..." — if that's what you're testing, you can't assume it. Fix: Start from neutral ground.

### 5. Ignoring Surviving Problems
Your preferred conclusion survives, but has problems. Ignoring them is dishonest. Fix: State problems on your own side.

### 6. No Control Cases
Everything comes out exotic/interesting. Fix: Include cases where mundane should win.

---

## Meta-Level Practices

### Across Multiple Questions

When you apply constraint stacking to many questions, patterns emerge:

- Do certain categories consistently resist mundane explanation?
- Do conclusions point in consistent directions?
- What would it mean if they do?

The pattern across analyses can be more informative than any single analysis.

### Predictions and Falsifiability

Strong frameworks predict. After reaching conclusions, ask:

- What would we expect to see if this is right?
- What would we expect NOT to see?
- What specific finding would break this picture?

State these explicitly. If predictions fail, update.

### Steelmanning Objections

The strongest objections to your conclusions deserve direct response:

1. State the objection as strongly as possible
2. Acknowledge what's legitimate about it
3. Respond specifically
4. Concede if you can't fully address it

This builds trust and sharpens thinking.

---

## When to Use This Method

**Good for:**
- Questions with multiple competing explanations
- Questions where evidence exists but is contested
- Questions where you want to test your own assumptions
- Questions where mainstream and heterodox views both have defenders

**Not ideal for:**
- Pure value questions (ethics, aesthetics)
- Questions with no evidence either way
- Questions where one answer is already proven beyond doubt

---

## Quick Reference

1. **Define question precisely**
2. **List all explanations**
3. **Order evidence by eliminative power** (anomalies > limits > absence > priors)
4. **Apply constraints top-down**
5. **Test BOTH directions**
6. **Weigh which side has worse problems**
7. **Report what survives with appropriate uncertainty**
8. **Include control cases**
9. **State what would break your conclusion**
10. **Steelman the best objections**

---

## The Deepest Lesson

The method doesn't tell you what's true. It tells you what survives elimination.

"Cannot eliminate" is not "proven true." It's "still standing after we threw everything at it."

That's epistemic humility combined with epistemic rigor. Not certainty, but calibrated confidence based on what the evidence actually supports.

---

*Method refined through application to 36 questions across consciousness, physics, UAP, history, and foundations. Control group confirms method debunks when debunking fits.*

---

<p><a href="{{ '/methodology/' | relative_url }}">← Back to Methodology</a></p>
