---
layout: page
title: Start Here
description: When to use the framework and quick-start guide
permalink: /start/
---

## When to Use This Framework

The Constraint Framework works best on **empirical questions with checkable predictions**.

### Use It When:

**✓ Physical evidence exists**
- Crime scenes, disease outbreaks, accidents
- Can create hard constraints
- Predictions are checkable
- Reality provides clear answers

**✓ Multiple independent witnesses**
- Can check for convergence
- Inconsistencies reveal false hypotheses
- Consistency supports true hypotheses

**✓ Known comparison cases**
- Can establish base rates
- Can identify patterns
- Can check against benchmarks

**✓ Checkable predictions**
- Hypothesis makes specific claims
- Claims can be verified or falsified
- Timeline allows checking

**✓ Multiple competing hypotheses**
- Comparative strength is measurable
- Can rank options
- Clear winner can emerge

### Don't Use It When:

**✗ No physical evidence**
- Everything becomes speculation
- Can't create hard constraints
- Example: Loch Ness Monster (no body, no clear photos)

**✗ Single witness, no corroboration**
- Can't check convergence
- Testimony could be accurate or false
- No way to differentiate

**✗ Evidence destroyed or lost**
- Can't check predictions
- Must rely on incomplete records
- Ambiguity increases

**✗ Unfalsifiable hypotheses**
- No predictions to test
- Can't be proven wrong
- Framework is helpless
- Examples: Simulation theory, solipsism

**✗ Future predictions**
- Can't check yet
- Framework requires checking against reality
- Must wait

**✗ Purely philosophical questions**
- No empirical predictions
- No observable differences between answers
- Outside framework's scope
- Examples: "What is consciousness?" "Do we have free will?"

---

## Quick-Start Checklist

Before starting, verify:

- [ ] Mystery has observable facts to check
- [ ] Multiple competing explanations exist
- [ ] Predictions can be verified
- [ ] Evidence is available
- [ ] You're willing to follow where evidence leads

**If any checkbox fails, framework may not be appropriate.**

---

## The 7-Step Process

### Step 1: Define the Mystery

Identify the core observation requiring explanation.

**Good example:**
"Why did SARS-CoV-2 emerge in Wuhan in late 2019?"

**Bad example:**
"Is there life in the universe?" (too broad, not specific)

**Key criterion:** Must have observable facts that can differentiate between explanations.

### Step 2: List All Serious Hypotheses

Brainstorm every explanation that:
- Has been proposed by credible sources
- Has some initial plausibility
- Makes different predictions than alternatives

**Include your preferred answer to test it fairly.**

### Step 3: Generate Predictions

For each hypothesis, ask: "If this were true, what would we EXPECT to observe?"

**Prediction categories:**
- Physical evidence
- Witness behavior
- Timeline/sequence
- Comparative benchmarks
- Perpetrator behavior
- Follow-up consequences

**Rules for good predictions:**
- ✓ Specific: "Ransom note written on house materials"
- ✓ Checkable: Can verify yes/no
- ✓ Non-circular: Can't use hypothesis to predict itself
- ✓ Falsifiable: If wrong, we can tell

### Step 4: Check Predictions

For each prediction:

**PASS (✓):** Prediction confirmed by evidence

**FAIL (✗):** Prediction contradicted by evidence

**UNCLEAR (⚠️):** Evidence insufficient or ambiguous

**Classify failure severity:**
- **Minor (-):** Prediction was "possibly" or "might"
- **Major (--):** Strong prediction failed, hard to explain
- **Critical (---):** Absolute necessity for hypothesis, failure proves it false

### Step 5: Score Each Hypothesis

```
Success Rate = Confirmed Predictions / Total Predictions

Severity Penalty = (Minor × 0.1 + Major × 0.3 + Critical × 0.6) / Total

Overall Fit = Success Rate × (1 - Severity Penalty)
```

**Interpretation:**
- 90-100%: Overwhelming support
- 70-89%: Strong support
- 50-69%: Moderate support
- 30-49%: Weak support
- 0-29%: Very weak/disproven

### Step 6: Compare Hypotheses

```
Advantage Ratio = Best Hypothesis Fit / Next Best Fit
```

**Interpretation:**
- 100:1+: Overwhelming winner
- 20:1 to 99:1: Very strong winner
- 10:1 to 19:1: Strong winner
- 5:1 to 9:1: Moderate winner
- 2:1 to 4:1: Weak winner
- <2:1: Too close to call

### Step 7: Check Constraints

List major constraints and which hypothesis they support.

**Do constraints converge?**
- If yes → Strong conclusion
- If mixed → Describe the conflict
- If no → Evidence is genuinely ambiguous

---

## Example: Quick Analysis

**Mystery:** JonBenét Ramsey ransom note

**Hypotheses:**
- H1: Stranger intruder wrote it
- H2: Parent wrote it (cover-up)

**Key Predictions:**

**Intruder hypothesis predicts:**
- Uses own materials (✗ used house notepad)
- Standard ransom amount (✗ exactly John's bonus: $118,000)
- Brief note (✗ 2.5 pages, longest in FBI history)
- Takes note with them (✗ left at scene)

**Parent hypothesis predicts:**
- Uses house materials (✓ Patsy's notepad)
- Knows inside information (✓ exact bonus amount)
- Anxious, lengthy note (✓ 2.5 pages)
- Staging indicators (✓ practice note found)

**Result:** Parent hypothesis 100% fit, intruder hypothesis <1% fit

**Advantage ratio:** >100:1 for parent involvement

---

## Next Steps

**Understand the full methodology:**
<p><a href="{{ '/methodology/' | relative_url }}">Complete framework →</a></p>

**Learn the scoring system:**
<p><a href="{{ '/scoring/' | relative_url }}">How scoring works →</a></p>

**See it applied:**
<p><a href="{{ '/mysteries/' | relative_url }}">Case studies →</a></p>

---

## Key Principle

**The framework doesn't make you smarter. It makes your biases visible.**

It forces intellectual honesty. You can't cherry-pick, ignore contradictions, or hide behind "we can't know."

You must list all predictions upfront, check each honestly, count failures, compare fairly, and state confidence accurately.

That's the whole point.
