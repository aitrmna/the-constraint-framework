---
layout: page
title: Methodology
description: The complete Constraint Framework
permalink: /methodology/
---

## The Nucleus

**True hypotheses constrain reality. False ones don't.**

If something is TRUE:
- Predicts what we observe
- Doesn't predict what's absent

If something is FALSE:
- Predicts things that aren't there, OR
- Fails to predict things that are

Reality kills false hypotheses. The more predictions, the more chances for reality to expose falsehood.

---

## The Process

### 1. List Competing Hypotheses

All serious explanations for the phenomenon.

**Example (JonBenét):**
- Stranger intruder
- Parent rage killing  
- Parent sexual abuse
- Burke accident + cover-up
- Failed kidnapping

### 2. For Each Hypothesis: What Must Be True?

**Ask:**
- "What would we HAVE to see if this were correct?"
- "What would we NEVER see if this were correct?"

**Example (Intruder hypothesis):**
- MUST see: Signs of forced entry, unknown DNA, stranger behavior patterns
- NEVER see: Ransom note on victim's notepad, amount matching father's bonus, hours-long staging

Make predictions **before** checking evidence. Otherwise you're rationalizing, not testing.

### 3. Check Reality

For each prediction:

**✓ PASS** — Evidence confirms
**✗ FAIL** — Evidence contradicts
**⚠️ UNCLEAR** — Insufficient data

**Weight failures by severity:**

**MINOR (-)** — "Might" predictions, 10% penalty
- Doesn't fundamentally damage hypothesis
- Example: "Might find fingerprints" → None found

**MAJOR (--)** — Strong predictions, 30% penalty  
- Central to hypothesis, hard to explain
- Example: "Should find animal reservoir within year" → None after 5 years

**CRITICAL (---)** — Absolute necessities, 60% penalty
- No alternative explanation, proves it false
- Example: "Radar tracked objects" → Radar tracked F-16s tracking each other

### 4. Score and Compare

**Scoring:**
```
Success Rate = Passes / Total
Severity Penalty = (Minor×0.1 + Major×0.3 + Critical×0.6) / Total  
Overall Fit = Success Rate × (1 - Severity Penalty)
```

**Compare:**
```
Advantage = Best Fit / Next Best Fit
```

**Interpretation:**
- 100:1+ — Alternative essentially impossible
- 20:1 to 99:1 — Alternative highly improbable  
- 10:1 to 19:1 — Alternative unlikely
- 5:1 to 9:1 — Alternative possible but less likely
- 2:1 to 4:1 — Too close to call

**Example:**
- Burke scenario: 100% fit (zero failures)
- Intruder: 0.5% fit (multiple critical failures)
- Ratio: 200:1 for Burke

---

## Worked Example: Belgian UFO Wave

**Claim:** Extraordinary craft observed over Belgium

**Predictions:**
1. Radar tracked real objects → **CRITICAL FAIL** (tracked F-16s tracking each other)
2. Pilots visually confirmed → **CRITICAL FAIL** (saw nothing during locks)
3. Photos/video exist → **CRITICAL FAIL** (one hoax photo, no video)
4. Contemporary media footage → **MAJOR FAIL** (none found online)
5. Reports on date of sightings → **CRITICAL FAIL** (zero on date, 143 after publicity)

**Score:**
- Passes: 0/5
- Critical failures: 4
- Major failures: 1
- Overall fit: 0%

**Result:** Hypothesis eliminated. Multiple critical failures.

---

## Why This Works

**Foundation:** Karl Popper's falsification

- Theories can be proven false
- Can't be proven absolutely true
- Science advances by elimination
- Strong theories make risky predictions that could fail

**This framework:**
- Forces risky predictions (before checking)
- Counts failures honestly (no cherry-picking)
- Weights severity (critical vs minor)
- Compares numerically (objective)

True hypotheses survive contact with reality. False ones don't.

---

## Common Mistakes

**1. Checking evidence first**
- Generates post-hoc predictions
- Always seems to confirm your hunch
- Solution: Write predictions, lock them in, THEN check

**2. Vague predictions**
- "Something unusual might happen"
- Can't fail = doesn't help
- Solution: Force specificity. What exactly? Where? When?

**3. Ignoring failures**
- "That failure doesn't really count because..."
- Moving goalposts after the fact
- Solution: Assign severity BEFORE checking. No revising.

**4. Unfalsifiable claims**
- "Too advanced to detect"
- "Absence of evidence isn't evidence of absence"
- Solution: If nothing could prove it wrong, it's not testable

**5. Treating scores as truth**
- "85% means it's true"
- False precision
- Solution: Scores show relative strength only, not certainty

---

## When It Works

**Best:**
- Physical evidence
- Multiple witnesses
- Checkable predictions
- Known comparison cases

**Struggles:**
- No evidence
- Single witness
- Evidence destroyed
- Unfalsifiable claims
- Pure philosophy

---

## Core Principles

1. Predictions before evidence
2. Count failures honestly
3. Weight severity objectively
4. Compare numerically
5. Follow evidence over preference

---

## The One Line

**"What does this predict? Does reality match?"**

---

<p><a href="{{ '/scoring/' | relative_url }}">Scoring details →</a></p>
<p><a href="{{ '/mysteries/' | relative_url }}">Case studies →</a></p>
<p><a href="{{ '/start/' | relative_url }}">When to use this →</a></p>
