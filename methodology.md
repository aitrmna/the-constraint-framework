---
layout: page
title: Methodology
description: The complete Constraint Framework
permalink: /methodology/
---

## Core Philosophy

### The Traditional Problem

**Standard reasoning failures:**
1. **Possibility bias:** "It's possible aliens visited" → Stops there
2. **Confirmation bias:** Looks only for supporting evidence
3. **Motivated reasoning:** Reaches desired conclusion
4. **False equivalence:** Treats all explanations as equally valid
5. **Extraordinary claims fallacy:** Subjective bar for what counts as "extraordinary"

**What we actually need:** Objective way to compare competing explanations.

### The Constraint Framework Solution

**Core insight:** Every hypothesis, if true, PREDICTS specific observable consequences.

**The test:**
1. List what hypothesis predicts
2. Check each prediction against reality
3. Count confirmations vs failures
4. Weight severity of failures
5. Compare scores across hypotheses

**Result:** Objective, quantifiable, comparative evaluation.

---

## The 7-Step Process

### STEP 1: Define the Mystery

**Identify the core observation requiring explanation.**

**Good examples:**
- "Why did SARS-CoV-2 emerge in Wuhan in late 2019?"
- "How did JonBenét Ramsey die and who is responsible?"
- "What did Navy pilots observe on November 14, 2004?"

**Bad examples:**
- "Is there life in the universe?" (too broad)
- "What is consciousness?" (philosophical, not empirical)
- "Will AI destroy humanity?" (future prediction, not past explanation)

**Key criterion:** Must have observable facts that can differentiate between explanations.

---

### STEP 2: List All Serious Hypotheses

**Brainstorm every explanation that:**
- Has been proposed by credible sources
- Has some initial plausibility
- Makes different predictions than alternatives

**Include:**
- Conventional wisdom
- Minority expert opinions
- Unpopular but logical possibilities
- Deliberately include your "preferred" answer to test it fairly

**Exclude:**
- Logically impossible explanations
- Explanations that make identical predictions to others (combine them)
- Purely ad-hoc explanations with no predictive power

**Example (JonBenét):**
- H1: Stranger intruder
- H2: Patsy rage killing
- H3: John sexual abuse
- H4: Failed kidnapping
- H5: Burke accident + parent cover-up

---

### STEP 3: Generate Predictions for Each Hypothesis

**For each hypothesis, ask: "If this were true, what would we EXPECT to observe?"**

**Prediction categories:**

**Physical evidence:**
- What materials should be present?
- What should be absent?
- What patterns in evidence?

**Witness behavior:**
- How many witnesses?
- Consistency of accounts?
- Changes over time?

**Timeline/sequence:**
- What order of events?
- What timing intervals?
- What correlations?

**Comparative benchmarks:**
- How does this compare to known similar cases?
- What do we see in verified examples?
- What's the base rate?

**Perpetrator behavior:**
- What actions would they take?
- What would they avoid?
- What capabilities required?

**Follow-up consequences:**
- What happens next?
- What evidence emerges over time?
- How do actors behave later?

**Rules for good predictions:**

✓ **Specific:** "Ransom note written on materials from house" not "there might be a note"
✓ **Checkable:** Can verify yes/no against evidence
✓ **Non-circular:** Can't use the hypothesis to predict itself
✓ **Falsifiable:** If wrong, we can tell

✗ **Avoid:**
- Vague predictions ("something unusual might occur")
- Unfalsifiable claims ("absence of evidence isn't evidence of absence")
- Circular reasoning ("it's aliens because it's unexplained")

---

### STEP 4: Check Predictions Against Reality

**For each prediction, determine:**

**PASS (✓):** Prediction confirmed by evidence
- Evidence directly supports this prediction
- Multiple independent sources confirm
- No contradicting evidence

**FAIL (✗):** Prediction contradicted by evidence  
- Evidence directly contradicts prediction
- Multiple sources show opposite
- Contradiction is clear and unambiguous

**UNCLEAR (⚠️):** Evidence insufficient or ambiguous
- No data available
- Conflicting reports
- Interpretation disputed

**Severity classification for failures:**

**MINOR failure (-):**
- Prediction was "possibly" or "might"
- Failure doesn't fundamentally undermine hypothesis
- Other factors could explain
- Example: "Might find fingerprints" → None found

**MAJOR failure (--):**
- Strong prediction that failed
- Central to hypothesis but alternatives exist
- Hard to explain away
- Example: "Should find animal reservoir within year" → None after 5 years

**CRITICAL failure (---):**
- Prediction was absolute necessity for hypothesis
- No alternative explanation
- Failure essentially proves hypothesis false
- Example: "Radar tracked actual objects" → Radar tracked F-16s tracking each other

---

### STEP 5: Score Each Hypothesis

**Basic scoring:**

```
Success Rate = (Confirmed Predictions) / (Total Predictions)
```

**Severity adjustment:**

```
Severity Penalty = (Minor Failures × 0.1) + (Major Failures × 0.3) + (Critical Failures × 0.6)
                   ────────────────────────────────────────────────────────────────────────
                                        Total Predictions
```

**Overall fit:**

```
Overall Fit = Success Rate × (1 - Severity Penalty)
```

**Interpretation scale:**

- **90-100%:** Overwhelming support, near certainty
- **70-89%:** Strong support, high confidence
- **50-69%:** Moderate support, probable
- **30-49%:** Weak support, possible but unlikely
- **10-29%:** Very weak support, improbable
- **0-9%:** Essentially disproven

---

### STEP 6: Compare Hypotheses

**Calculate relative strength:**

```
Advantage Ratio = (Best Hypothesis Overall Fit) / (Next Best Overall Fit)
```

**Interpretation:**

- **100:1 or greater:** Overwhelming winner, alternative essentially impossible
- **20:1 to 99:1:** Very strong winner, alternatives highly improbable
- **10:1 to 19:1:** Strong winner, alternatives unlikely
- **5:1 to 9:1:** Moderate winner, alternatives possible but less likely
- **2:1 to 4:1:** Weak winner, alternatives remain plausible
- **Less than 2:1:** Too close to call, evidence insufficient

**Examples from case studies:**

| Mystery | Winner | Score | Next Best | Score | Ratio |
|---------|--------|-------|-----------|-------|-------|
| COVID Origin | Lab Leak | 100% | Natural | 0.75% | 133:1 |
| JonBenét | Burke+Cover-up | 100% | Patsy Rage | 26% | 4:1* |
| D.B. Cooper | Died | 95% | Survived | 1% | 95:1 |
| MH370 | Pilot Suicide | 90% | Hijacking | 10% | 9:1 |
| Tic Tac | Non-human | 60% | Secret Tech | 5% | 12:1 |

*But 200:1 vs intruder theory

---

### STEP 7: Apply Bayesian Reasoning

**Update probabilities based on evidence strength.**

**Prior probability:** What we'd think before seeing evidence
**Likelihood ratio:** How much more likely evidence is under one hypothesis vs another
**Posterior probability:** Updated belief after seeing evidence

**Formula:**

```
P(H|E) = P(E|H) × P(H) / P(E)

Where:
P(H|E) = Probability of hypothesis given evidence
P(E|H) = Probability of evidence given hypothesis (from our predictions)
P(H) = Prior probability of hypothesis
P(E) = Total probability of evidence
```

**Simplified approach:**

```
Posterior Odds = Prior Odds × Likelihood Ratio

Where Likelihood Ratio = P(E|H1) / P(E|H2)
```

**Practical example (JonBenét):**

**Evidence:** Ransom note written on Patsy's notepad, 2.5 pages, $118k = John's bonus

**P(Evidence | Intruder):**
- Uses victim's notepad: 1%
- Knows exact bonus: 0.1%
- Writes 2.5 pages: 1%
- Combined: ~0.00001 (1 in 100,000)

**P(Evidence | Parent Cover-up):**
- Uses own notepad: 99%
- Knows own bonus: 100%
- Long note (staging): 30%
- Combined: ~0.3 (30%)

**Likelihood Ratio:** 0.3 / 0.00001 = 30,000:1 in favor of parent involvement

**This ONE piece of evidence shifts probability by factor of 30,000.**

---

## Constraint Stacking

### What Are Constraints?

**Constraint:** An observable fact that a true hypothesis MUST explain.

**Hard constraint:** Cannot be violated (physical laws, logical necessities)
**Soft constraint:** Can be violated but at cost to hypothesis (statistical patterns, typical behaviors)

### The Power of Multiple Constraints

**Key insight:** Multiple independent constraints multiply against false hypotheses.

**Example (Belgian UFO Wave):**

**CONSTRAINT 1:** Radar performance data
- Reality: Radar locked on F-16s tracking each other
- **FAILED:** Foundation evidence collapses

**CONSTRAINT 2:** Visual confirmation
- Reality: Pilots saw nothing during radar locks
- **FAILED:** No visual confirmation of claimed performance

**CONSTRAINT 3:** Photographic evidence
- Reality: ONE photo (admitted hoax in 2011), no video found
- **FAILED:** Missing expected documentation

**CONSTRAINT 4:** Media coverage
- Reality: Essentially no contemporary footage online
- **FAILED:** Coverage claims unsupported

**CONSTRAINT 5:** Report timing
- Reality: ZERO reports on date, 143 over next 2 weeks after media solicitation
- **FAILED:** Reports followed publicity, not sightings

**Result:** Each failed constraint weakens hypothesis. Multiple failures compound. Five critical failures = case collapses.

### How to Stack Constraints

**1. Identify independent observables:**
- Don't just list "evidence" 
- List distinct types of evidence that test different aspects
- Ensure they're truly independent

**2. For each constraint, ask:**
- What does Hypothesis A predict?
- What does Hypothesis B predict?
- What do we actually observe?
- Which hypothesis does observation support?

**3. Build the stack:**
- Start with strongest, clearest constraints
- Add progressively weaker ones
- Note when constraints align (supporting same conclusion)
- Note when constraints conflict (evidence is genuinely ambiguous)

**4. Check for:**
- **Convergence:** Do multiple independent constraints point same direction?
- **Contradiction:** Do constraints point different directions?
- **Gaps:** Are there untested aspects?

**Example (COVID Origin):**

**6 Major Constraints:**

1. **Animal Reservoir:** Lab leak predicts none found. Natural predicts found quickly.
   - **Observation:** None found in 5 years, 80,000 animals tested
   - **Points to:** Lab leak

2. **Furin Cleavage Site:** Lab leak predicts engineered feature. Natural predicts found in relatives.
   - **Observation:** Never seen in 3,000+ related viruses
   - **Points to:** Lab leak

3. **Geographic Proximity:** Lab leak predicts outbreak near WIV. Natural predicts near wildlife reservoir.
   - **Observation:** 8 miles from WIV, 1,000 miles from closest related virus
   - **Points to:** Lab leak

4. **Immediate Optimization:** Lab leak predicts ready for humans. Natural predicts adaptation period.
   - **Observation:** Immediately optimized, no intermediates
   - **Points to:** Lab leak

5. **Chinese Government Behavior:** Lab leak predicts cover-up. Natural predicts transparency.
   - **Observation:** Evidence destroyed, investigation blocked, scientists pressured
   - **Points to:** Lab leak

6. **WIV Capabilities:** Lab leak predicts relevant research. Natural predicts irrelevant.
   - **Observation:** GOF research, right viruses, safety concerns, database offline, researchers hospitalized
   - **Points to:** Lab leak

**All 6 constraints align → Overwhelming case for lab leak**

---

## Common Pitfalls

### Pitfall 1: Cherry-Picking Evidence

**What it looks like:**
- Only listing predictions that pass
- Ignoring failed predictions

**How to avoid:**
- Generate ALL predictions before checking any
- Force yourself to list predictions you know will fail
- Check: "What would disprove this hypothesis?"

### Pitfall 2: Vague Predictions

**What it looks like:**
- "Something unusual might occur"
- "It's possible that..."

**How to avoid:**
- Force specificity: What exactly? Where? When? How much?
- Quantify when possible
- Make falsifiable

### Pitfall 3: Moving Goalposts

**What it looks like:**
- Prediction fails → "Well, that's not really essential"
- Adding new hypotheses to explain away failures

**How to avoid:**
- Write down predictions BEFORE checking
- Commit to severity ratings upfront
- If you revise hypothesis, restart from Step 2

### Pitfall 4: Unfalsifiable Hypotheses

**What it looks like:**
- "Aliens are too advanced to detect"
- "Absence of evidence isn't evidence of absence"

**How to avoid:**
- Ask: "What observation would prove this wrong?"
- If answer is "nothing," it's unfalsifiable
- Unfalsifiable = untestable = not useful

### Pitfall 5: False Precision

**What it looks like:**
- "Hypothesis is 73.4% likely to be true"
- Over-relying on exact percentages

**How to avoid:**
- Use scores for COMPARISON, not absolute truth
- Round to confidence brackets
- Acknowledge uncertainty

### Pitfall 6: Ignoring Priors

**What it looks like:**
- Treating all hypotheses as equally likely before evidence
- Starting with 50/50 odds regardless of plausibility

**How to avoid:**
- Consider base rates
- Use Bayesian priors
- Adjust evidence strength by prior probability

---

## When the Framework Works vs. Doesn't Work

### Framework Works Best When:

✓ Physical evidence exists
✓ Multiple independent witnesses
✓ Known comparison cases
✓ Checkable predictions
✓ Multiple competing hypotheses

### Framework Struggles When:

✗ No physical evidence
✗ Single witness, no corroboration
✗ Evidence destroyed or lost
✗ Unfalsifiable hypotheses
✗ Future predictions
✗ Purely philosophical questions

---

## Epistemological Foundations

### Why This Works

**Core insight from Karl Popper:**
- Theories can't be proven true
- Theories CAN be proven false
- Science advances by elimination
- Strong theories risk being wrong (make bold predictions)

**Our application:**
- Good hypotheses make risky predictions
- Failed predictions eliminate hypotheses
- Surviving hypotheses are stronger
- Not "proved true" but "not yet proven false"

### Connection to Bayesian Reasoning

- Start with prior probabilities
- Update based on evidence
- Each piece of evidence has likelihood ratio
- Posterior probability = prior × likelihood ratio
- Framework systematizes this

### Epistemic Humility

**What we can know:**
- Relative strength of hypotheses
- Which predictions pass/fail
- Confidence levels

**What we can't know:**
- Absolute certainty (almost never achievable)
- Future developments
- Evidence we don't have

**The framework is honest about both.**

---

## Next Steps

**Learn the scoring system:**
<p><a href="{{ '/scoring/' | relative_url }}">Detailed scoring guide →</a></p>

**See it applied:**
<p><a href="{{ '/mysteries/' | relative_url }}">Case studies →</a></p>

**Understand when to use it:**
<p><a href="{{ '/start/' | relative_url }}">Quick-start guide →</a></p>

---

## Core Principles (Memorize These)

1. **Strong hypotheses predict everything observed, predict nothing not observed**
2. **Generate predictions before checking evidence**
3. **Count both successes and failures**
4. **Weight severity of failures**
5. **Compare hypotheses numerically**
6. **Follow evidence over preference**
7. **State confidence honestly**
8. **Update when new evidence emerges**

---

## The One-Sentence Summary

**"What does this hypothesis predict, and does reality match?"**
