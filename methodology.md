---
layout: page
title: Methodology
description: The complete Constraint Framework
permalink: /methodology/
---

## The Nucleus

**True hypotheses constrain reality. False ones don't.**

If something is TRUE:
- It predicts what we observe
- It doesn't predict what's absent

If something is FALSE:
- It predicts things that aren't there, OR
- It fails to predict things that are there

**Reality kills false hypotheses.** The more predictions you make, the more opportunities for reality to expose falsehood.

---

## The Operational Form

Ask two questions about every hypothesis:

1. **"What would we HAVE to see if this were true?"** → Check if present
2. **"What would we NEVER see if this were true?"** → Check if absent

If reality contradicts either answer, the hypothesis is damaged or eliminated.

**This is falsifiability weaponized.**

---

## The 7-Step Process

### STEP 1: Define the Mystery

Identify the core observation requiring explanation.

**Good examples:**
- "What did Navy pilots observe on November 14, 2004?"
- "Why did SARS-CoV-2 emerge in Wuhan in late 2019?"
- "How did JonBenét Ramsey die and who is responsible?"

**Key criterion:** Observable facts that can differentiate between explanations.

---

### STEP 2: List All Serious Hypotheses

**Include:**
- Conventional wisdom
- Minority expert opinions
- Unpopular but logical possibilities
- Your "preferred" answer (to test it fairly)

**Exclude:**
- Logically impossible explanations
- Explanations making identical predictions (combine them)
- Purely ad-hoc explanations with no predictive power

**Example (JonBenét):**
- H1: Stranger intruder
- H2: Patsy rage killing
- H3: John sexual abuse
- H4: Failed kidnapping attempt
- H5: Burke accident + parent cover-up

---

### STEP 3: Generate Predictions for Each Hypothesis

**For each hypothesis, ask: "If this were true, what would we EXPECT to observe?"**

**Prediction categories:**
- Physical evidence: What materials? What absent? What patterns?
- Witness behavior: How many? Consistency? Changes over time?
- Timeline: What sequence? What timing? What correlations?
- Comparative benchmarks: How does this compare to known similar cases?
- Perpetrator behavior: What actions? What avoided? What capabilities needed?
- Follow-up consequences: What happens next? What evidence emerges later?

**Rules for good predictions:**

✓ **Specific:** "Ransom note written on materials from house" not "there might be a note"
✓ **Checkable:** Can verify yes/no against evidence
✓ **Risky:** If wrong, you can tell
✓ **Non-circular:** Can't use the hypothesis to predict itself

✗ **Avoid:**
- Vague predictions ("something unusual might occur")
- Unfalsifiable claims ("absence of evidence isn't evidence of absence")
- Circular reasoning ("it's aliens because it's unexplained")

**CRITICAL:** Generate predictions BEFORE checking evidence. This prevents post-hoc rationalization.

---

### STEP 4: Check Predictions Against Reality

For each prediction:

**PASS (✓):** Prediction confirmed
- Evidence directly supports this prediction
- Multiple independent sources confirm
- No contradicting evidence

**FAIL (✗):** Prediction contradicted
- Evidence directly contradicts prediction
- Multiple sources show opposite
- Contradiction is clear and unambiguous

**UNCLEAR (⚠️):** Insufficient evidence
- No data available
- Conflicting reports
- Interpretation disputed

**Classify failure severity:**

**MINOR failure (-):** 10% penalty
- Prediction was weak ("possibly" or "might")
- Failure doesn't fundamentally undermine hypothesis
- Other factors could explain
- Example: "Might find fingerprints" → None found

**MAJOR failure (--):** 30% penalty
- Strong prediction that failed
- Central to hypothesis but alternatives exist
- Hard to explain away
- Example: "Should find animal reservoir within year" → None after 5 years

**CRITICAL failure (---):** 60% penalty
- Prediction was absolute necessity
- No alternative explanation
- Failure essentially proves hypothesis false
- Example: "Radar tracked actual objects" → Radar tracked F-16s tracking each other

---

### STEP 5: Score Each Hypothesis

**Success Rate:**
```
Success Rate = Confirmed Predictions / Total Predictions
```

**Severity Penalty:**
```
Severity Penalty = (Minor × 0.1) + (Major × 0.3) + (Critical × 0.6)
                   ─────────────────────────────────────────────────
                                 Total Predictions
```

**Overall Fit:**
```
Overall Fit = Success Rate × (1 - Severity Penalty)
```

**Interpretation:**
- 90-100%: Overwhelming support
- 70-89%: Strong support
- 50-69%: Moderate support
- 30-49%: Weak support
- 10-29%: Very weak support
- 0-9%: Essentially disproven

---

### STEP 6: Compare Hypotheses

**Advantage Ratio:**
```
Advantage Ratio = Best Hypothesis Overall Fit / Next Best Overall Fit
```

**Interpretation:**
- **100:1+** — Alternative essentially impossible
- **20:1 to 99:1** — Alternative highly improbable
- **10:1 to 19:1** — Alternative unlikely
- **5:1 to 9:1** — Alternative possible but less likely
- **2:1 to 4:1** — Alternative remains plausible
- **<2:1** — Too close to call

**Example:**
- Lab Leak: 100% vs Natural Origin: 0.75% = **133:1 for lab leak**
- Burke+Cover-up: 100% vs Intruder: 0.5% = **200:1 for Burke**

---

### STEP 7: State Confidence Honestly

Scores show **relative strength**, not absolute truth.

**Report:**
- Which hypothesis fits evidence best
- By what margin
- What uncertainties remain
- What new evidence would change the conclusion

**Don't claim:**
- Absolute certainty (almost never achievable)
- Future developments
- Knowledge you don't have

---

## Worked Example

### Belgian UFO Wave (Extraordinary Craft Hypothesis)

**Predictions:**

1. Radar tracked real objects → **CRITICAL FAIL** (tracked F-16s tracking each other)
2. Visual confirmation by pilots → **CRITICAL FAIL** (pilots saw nothing during radar locks)
3. Photos/videos exist → **CRITICAL FAIL** (one hoax photo only, admitted 2011)
4. Contemporary media footage → **MAJOR FAIL** (essentially none found online)
5. Reports on sighting date → **CRITICAL FAIL** (zero on date, 143 after publicity)

**Scoring:**
- Total predictions: 5
- Confirmed: 0
- Critical failures: 4
- Major failures: 1

Success Rate: 0/5 = 0%
Severity Penalty: (0.3 + 2.4)/5 = 54%
Overall Fit: 0% × (1 - 0.54) = **0%**

**Result:** Hypothesis essentially disproven. Multiple critical failures.

---

## Why This Works

**Foundation: Karl Popper's falsificationism**
- Theories can't be proven true
- Theories CAN be proven false
- Science advances by elimination
- Strong theories make risky predictions

**Application:**
- Good hypotheses make risky predictions
- Failed predictions eliminate hypotheses
- Surviving hypotheses are stronger
- Not "proved true" but "not yet proven false"

**The framework systematizes this** with:
- Forced prediction before checking
- Rigorous counting (no cherry-picking)
- Severity weighting (not all failures equal)
- Numerical comparison (objective ranking)

---

## Common Pitfalls

### 1. Cherry-Picking Evidence

**What it looks like:**
- Only listing predictions that pass
- Ignoring failed predictions
- Dismissing failures as "not important"

**How to avoid:**
- Generate ALL predictions before checking any
- Force yourself to list predictions you know will fail
- Assign severity ratings BEFORE checking evidence

### 2. Vague Predictions

**What it looks like:**
- "Something unusual might occur"
- "It's possible that..."
- Predictions that can't fail

**How to avoid:**
- Force specificity: What exactly? Where? When? How much?
- Make falsifiable predictions
- If it can't be wrong, it doesn't help

### 3. Moving Goalposts

**What it looks like:**
- Prediction fails → "Well, that's not really essential"
- Adding new hypotheses to explain failures
- Redefining what counts as success

**How to avoid:**
- Write down predictions BEFORE checking
- Lock in severity ratings upfront
- If you revise hypothesis, restart from Step 2

### 4. Unfalsifiable Hypotheses

**What it looks like:**
- "Aliens are too advanced to detect"
- "Absence of evidence isn't evidence of absence"
- Can't specify what would prove it wrong

**How to avoid:**
- Ask: "What observation would prove this wrong?"
- If answer is "nothing," it's unfalsifiable
- Unfalsifiable = untestable = not useful

### 5. False Precision

**What it looks like:**
- "Hypothesis is 73.4% likely to be true"
- Over-relying on exact percentages
- Treating scores as absolute truth

**How to avoid:**
- Use scores for COMPARISON, not certainty claims
- Round to confidence brackets
- Acknowledge uncertainty
- Scores show relative strength only

---

## When the Framework Works vs. Doesn't

### Works Best When:

✓ Physical evidence exists
✓ Multiple independent witnesses
✓ Known comparison cases
✓ Checkable predictions possible
✓ Multiple competing hypotheses

### Struggles When:

✗ No physical evidence
✗ Single witness, no corroboration
✗ Evidence destroyed or lost
✗ Unfalsifiable hypotheses
✗ Future predictions
✗ Purely philosophical questions

**The framework is honest about both.**

---

## Core Principles (Memorize These)

1. **Generate predictions before checking evidence**
2. **Count both successes and failures**
3. **Weight severity objectively**
4. **Compare hypotheses numerically**
5. **Follow evidence over preference**
6. **State confidence honestly**
7. **Update when new evidence emerges**

---

## The One-Sentence Summary

**"What does this hypothesis predict, and does reality match?"**

---

## Next Steps

**Learn the scoring details:**
<p><a href="{{ '/scoring/' | relative_url }}">Detailed scoring guide →</a></p>

**See it applied:**
<p><a href="{{ '/mysteries/' | relative_url }}">Case studies →</a></p>

**Know when to use it:**
<p><a href="{{ '/start/' | relative_url }}">Quick-start guide →</a></p>

---

*The framework doesn't make you smarter. It makes your biases visible.*
