---
layout: page
title: Scoring System
description: How to score evidence nodes and calculate convergence.
permalink: /scoring/
---

This page explains how to score individual pieces of evidence and combine them into a convergence assessment.

**CRITICAL: This is observational, not prescriptive.**

---

## The Observational Principle

**We are not assigning scores based on what we want evidence to show.**  
**We are recognizing patterns that already exist in the evidence structure.**

When we say "Terminal Lucidity scores 9/10," we mean:
- Published in peer-reviewed journals (observable fact)
- Multiple independent observations (observable fact)
- Reproducible phenomenon (observable fact)
- No materialist mechanism proposed (observable gap)

**The score describes WHAT IS, not what we wish.**

### Second-Order Observation

The Evidence Convergence Framework predicts where evidence should concentrate:
- At boundaries
- At bottlenecks  
- In irreversible records
- At steep gradients
- Where independent streams meet

**Scoring observes: Did evidence concentrate where predicted?**

If yes → High score (prediction confirmed)  
If no → Low score (prediction failed)

This is the framework applied TO ITSELF. We're observing whether evidence actually concentrated at the structural inevitabilities the axioms predict.

---

## Evidence Node Scoring (0-10 scale)

Rate each piece of evidence based on OBSERVABLE characteristics:
1. How direct it is (observation > inference > speculation)
2. How verifiable it is (reproducible > documented > anecdotal)
3. How irreversible it is (permanent record > temporary state)

**These are descriptive categories, not value judgments.**

### The Scale

**9-10: Overwhelming**
- Multiple independent sensors/witnesses
- Physical artifacts that can't be explained away
- Reproducible experimental results
- Mathematical proofs

*Example: Thermodynamic laws (10/10) - tested billions of times, zero counterexamples*

**7-8: Strong**
- Single high-quality sensor/witness
- Documented physical evidence
- Well-replicated findings
- Clear irreversible signatures

*Example: Tic Tac radar tracks (8/10) - multiple sensors, recorded data, trained operators*

**5-6: Moderate**
- Credible but unverified reports
- Circumstantial but consistent evidence
- Limited replication
- Some ambiguity in interpretation

*Example: Single credible UFO witness (5/10) - honest reporter, but no physical evidence*

**3-4: Weak**
- Anecdotal reports
- Contradictory interpretations possible
- Poor documentation
- High noise-to-signal ratio

*Example: Urban legend (3/10) - story persists but details shift*

**1-2: Minimal**
- Hearsay
- No documentation
- Easily explained otherwise
- Likely false

*Example: Friend-of-a-friend story (1/10)*

**0: Non-evidence**
- Pure speculation
- Logically impossible
- Definitively disproven

---

## Testing Independence

Before combining evidence nodes, verify they're actually independent:

**Independent sources:**
- Different measurement methods (radar + visual)
- Different witnesses who couldn't coordinate (separated in space/time)
- Different research groups with different methods
- Cross-cultural patterns

**NOT independent:**
- Multiple news articles citing the same source
- Witnesses who talked to each other before reporting
- Studies using the same dataset
- All evidence interpreted through one framework

**The key test:** Can you eliminate one source and still reach the same conclusion? If yes, they're independent.

---

## Calculating Convergence Score

**Formula:** 
```
Convergence Score = (Number of independent streams) × (Average node strength)
```

**Example:**

Evidence for anthropogenic climate change:
- Ice core CO₂ data: 9/10
- Atmospheric measurements: 9/10  
- Ocean acidification: 8/10
- Isotope analysis: 9/10
- Glacier retreat: 8/10

Independence check: ✓ All use different measurement methods

Convergence = 5 streams × 8.6 avg = **43 points** (out of maximum ~100)

But wait - we also have:
- Radiative forcing calculations: 8/10
- Attribution studies: 7/10
- Biological range shifts: 7/10

Total: 8 independent streams × 8.1 avg = **65/100**

---

## Confidence Levels

Use convergence score to bracket confidence:

**90-100: Near-Certain**
- Multiple high-quality independent streams
- Would require coordinated failure across all sources
- Examples: Thermodynamics (99), Evolution (99), Heliocentrism (98)

**75-89: High Confidence**
- Several independent streams, mostly strong
- Alternative explanations strained
- Examples: Anthropogenic climate (85), Germ theory (87)

**60-74: Probable**
- Multiple streams but some weak, OR
- Few streams but very strong
- Reasonable alternatives exist but less supported

**40-59: Moderate**
- Mixed evidence quality
- Some independent confirmation
- Competing hypotheses viable

**20-39: Weak**
- Limited independent confirmation
- Mostly circumstantial
- Alternative explanations equally plausible

**0-19: Insufficient**
- Speculation
- Single sources only
- No independent verification

---

## Common Mistakes

**Mistake 1: Counting correlated sources as independent**

Wrong: "10 news articles about X" = 10 streams
Right: "10 news articles all citing study Y" = 1 stream

**Mistake 2: Averaging without checking independence**

Wrong: Average all evidence together
Right: Group correlated evidence, then combine groups

**Mistake 3: Confusing strength with relevance**

A piece of evidence can be high-quality (8/10) but irrelevant to the question.
Only score evidence that actually bears on the hypothesis.

**Mistake 4: Ignoring negative evidence**

Absence of evidence at a predicted bottleneck is ITSELF evidence.
Score it accordingly.

---

## What We Are NOT Doing

**NOT assigning scores based on desired conclusions:**
- We don't score Terminal Lucidity 9/10 because we want consciousness to survive death
- We score it 9/10 because it's peer-reviewed, reproducible, and unexplained by materialism (observable facts)

**NOT weighting evidence by philosophical preference:**
- We don't score brain-consciousness correlation 8/10 because it supports materialism
- We score it 8/10 because it's 100% experimentally consistent (observable fact)
- Then note it's compatible with BOTH production and filter theories (neutral)

**NOT cherry-picking which axioms apply:**
- We apply all six axioms to every case
- We note which ones succeed and which fail
- Hard Problem fails ALL axioms → Framework doesn't apply (honest observation)
- Tic Tac passes ALL axioms → Framework predicts high convergence (confirmed observation)

**NOT inventing the math:**
- Independent failures multiply: P(all wrong) = (1-p₁) × (1-p₂) × (1-p₃)
- This is probability theory, not our invention
- If 3 sources with 70% reliability agree: 1 - (0.3)³ = 97% confidence
- We're observing this mathematical relationship, not creating it

**NOT designing what convergence means:**
- Convergence = independent streams pointing same direction
- This is a pattern recognition, not a definition we invented
- We observe: Do independent research traditions reach same conclusion?
- If yes → convergence exists. If no → convergence absent.

**The entire scoring system is DESCRIPTIVE of patterns that exist in the knowledge structure, not PRESCRIPTIVE of what we want conclusions to be.**

---

## Worked Example: Tic Tac UAP

**Question:** Was the Tic Tac a physical object with anomalous capabilities?

**Evidence nodes:**

1. **Princeton radar tracks** - 8/10
   - Professional operators, recorded data, multiple detections
   - Axiom: Bottleneck (radar is mandatory checkpoint for physical objects)

2. **F/A-18 FLIR** - 7/10  
   - Thermal signature, locked target, recorded
   - Axiom: State-change amplification (if real, produces thermal signature)

3. **Commander Fravor visual** - 8/10
   - Trained pilot, clear conditions, prolonged observation
   - Axiom: Boundary conservation (contained in observable space)

4. **Lieutenant Slaight visual** - 7/10
   - Independent second witness, same conditions
   - Axiom: Convergence (independent observer)

5. **Multiple radar operators** - 7/10
   - Multiple independent confirmation of same tracks
   - Axiom: Convergence

6. **Physical performance (instantaneous acceleration)** - 9/10 IF real
   - Irreversible signature on multiple sensors
   - Axiom: Gradient navigation (extreme discontinuity from known aircraft)

**Independence check:**
- Radar ↔ FLIR: Independent sensors ✓
- Visual ↔ Radar: Independent observation methods ✓  
- Fravor ↔ Slaight: Separate aircraft ✓
- Performance data: Derived from above, not fully independent ✗

**Adjusted count:** 5 independent streams

**Convergence:** 5 × 7.7 avg = **38.5/100** ... wait, that seems low?

**Refinement:** The physical performance isn't separate evidence - it's what ALL the sensors converged on. So we should think of it as:

"5 independent high-quality sensors all detected the SAME anomalous object"

This is stronger than 5 random detections. The convergence ON ANOMALY is the key.

**Better framing:** 
- Convergence on "physical object existed": 95/100 (five sensors agree)
- Convergence on "anomalous performance": 70/100 (requires accepting sensor accuracy at extremes)

See [Tic Tac case study]({{ '/mysteries/tic-tac/' | relative_url }}) for full analysis.

---

## When NOT to Use Convergence Scoring

Don't use this for:
- Purely logical/mathematical questions (use proof instead)
- First-person subjective experience (no third-person convergence possible)
- Questions where "independent" sources don't exist
- Aesthetic or moral judgments

The framework is for empirical questions with observable evidence.

---

[Apply this → See mysteries scored]({{ '/mysteries/' | relative_url }})

[Back to Evidence Convergence →]({{ '/convergence/' | relative_url }})

[Back to Methodology →]({{ '/methodology/' | relative_url }})
